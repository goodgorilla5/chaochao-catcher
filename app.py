import streamlit as st
import pandas as pd
import re
import requests
from datetime import datetime

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="è¾²æœƒè¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

# è¾²æœƒå®šç¾©
FARMER_MAP = {"ç‡•å·¢": "S00076", "å¤§ç¤¾": "S00250", "é˜¿è“®": "S00098"}

# å“ç¨®å°ç…§è¡¨ (ä»£ç¢¼ -> ä¸­æ–‡å)
VARIETY_MAP = {
    "F22": "èœœæ£—",
    "FP1": "çç èŠ­",
    "FP2": "ç´…å¿ƒ",
    "FP3": "å¸ç‹èŠ­",
    "FP5": "æ°´æ™¶ç„¡ç±½",
    "FI3": "å…¶ä»–" # ä¿ç•™æ“´å……æ€§
}

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹è‡³ Streamlit å¾Œå° Secrets è¨­å®š github_token")
    st.stop()

def deep_parse(content):
    # ä½¿ç”¨æµæ°´è™Ÿç‰¹å¾µ [AT]11... é€²è¡Œåˆ‡å‰²
    records = re.split(r'(?=[AT]\d{10,})', content)
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    for rec in records:
        if not rec.strip(): continue
        try:
            # å°‹æ‰¾æ ¸å¿ƒéŒ¨é» (æ—¥æœŸ+ç­‰ç´š+S00)
            m = re.search(r'(\d{8})\s+(\d{2})(S00\d{6})', rec)
            if not m: continue
            
            raw_date = m.group(1)
            level_code = m.group(2)[0]
            market_anchor = m.group(3)
            serial = rec[:m.start()].strip().replace(" ", "")

            # æ•¸æ“šæ®µè§£æ
            data_part = rec[m.end():]
            if '+' not in data_part: continue
            parts = data_part.split('+')
            
            # æ•¸å€¼æå–
            pieces = int(parts[0][-3:].strip())
            weight = int(parts[1].strip())
            p_str = parts[2].strip().split()[0]
            price = int(p_str[:-1]) if p_str else 0
            
            # ç¸½åƒ¹ä¿ç•™
            t_str = parts[3].strip().split()[0]
            total_val = int(t_str) if t_str else 0
            
            # è²·å®¶æå–
            buyer_raw = parts[-1].strip()
            buyer_match = re.search(r'^\d+', buyer_raw)
            buyer = buyer_match.group() if buyer_match else ""

            # å“ç¨®æœå°‹èˆ‡è½‰æ›
            v_code_match = re.search(r'(F22|FP1|FP2|FP3|FP5|FI3)', parts[0])
            v_code = v_code_match.group(1) if v_code_match else "F22"
            v_name = VARIETY_MAP.get(v_code, v_code) # è½‰æ›ç‚ºä¸­æ–‡å

            # æ—¥æœŸè½‰å‹
            dt_obj = datetime(int(raw_date[:3])+1911, int(raw_date[3:5]), int(raw_date[5:7])).date()

            farm = "å…¶ä»–"
            for name, code in FARMER_MAP.items():
                if code in market_anchor: farm = name; break
            if farm == "å…¶ä»–": continue

            rows.append({
                "è¾²æœƒ": farm, "æ—¥æœŸ": dt_obj, "é¡¯ç¤ºæ—¥æœŸ": f"{raw_date[:3]}/{raw_date[3:5]}/{raw_date[5:7]}",
                "ç­‰ç´š": grade_map.get(level_code, level_code), "å°ä»£": market_anchor[6:9],
                "ä»¶æ•¸": pieces, "å…¬æ–¤": weight, "å–®åƒ¹": price, "ç¸½åƒ¹": total_val,
                "è²·å®¶": buyer, "æµæ°´è™Ÿ": serial, "å“ç¨®": v_name
            })
        except: continue
    return rows

@st.cache_data(ttl=60)
def fetch_data():
    all_rows = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get("https://api.github.com/repos/goodgorilla5/chaochao-catcher/contents/", headers=headers)
        files = [f for f in r.json() if f['name'].lower().endswith('.scp')]
        for f_info in files:
            res = requests.get(f_info['download_url'], headers=headers)
            all_rows.extend(deep_parse(res.content.decode("big5", errors="ignore")))
        
        full_df = pd.DataFrame(all_rows)
        # --- ğŸ›¡ï¸ æ•¸æ“šå»é‡é˜²ç¦¦ ---
        if not full_df.empty:
            full_df = full_df.drop_duplicates(
                subset=["æµæ°´è™Ÿ", "æ—¥æœŸ", "å°ä»£", "ä»¶æ•¸", "ç¸½åƒ¹", "è²·å®¶"], 
                keep='first'
            )
        return full_df
    except: return pd.DataFrame()

# --- ä¸»ä»‹é¢ ---
st.title("ğŸ è¾²æœƒè¡Œæƒ…å¤§æ•¸æ“šåº«")
df = fetch_data()

if not df.empty:
    # --- å´é‚Šæ¬„è¨­å®š ---
    st.sidebar.header("ğŸ¨ é¡¯ç¤ºè¨­å®š")
    show_grade = st.sidebar.checkbox("é¡¯ç¤ºç­‰ç´š", value=False)
    show_total = st.sidebar.checkbox("é¡¯ç¤ºç¸½åƒ¹", value=False)
    show_serial = st.sidebar.checkbox("é¡¯ç¤ºæµæ°´è™Ÿ", value=False)
    
    target_farm = st.selectbox("ğŸ¥ é¸æ“‡è¾²æœƒ", list(FARMER_MAP.keys()))
    
    # å“ç¨®é¸å–®ï¼šç¾åœ¨æœƒé¡¯ç¤º "èœœæ£—", "çç èŠ­" ç­‰ä¸­æ–‡åç¨±
    v_list = sorted(df[df['è¾²æœƒ']==target_farm]['å“ç¨®'].unique())
    default_v = "èœœæ£—" if "èœœæ£—" in v_list else v_list[0]
    target_v = st.selectbox("ğŸ é¸æ“‡å“ç¨®", v_list, index=v_list.index(default_v))
    
    # æ—¥æœŸå€é–“é¸æ“‡ (é è¨­æœ€æ–°å–®æ—¥)
    max_date = df['æ—¥æœŸ'].max()
    date_range = st.date_input("ğŸ“… é¸æ“‡æ—¥æœŸå€é–“", value=[max_date, max_date])

    # ç¯©é¸é‚è¼¯
    f_df = df[(df['è¾²æœƒ'] == target_farm) & (df['å“ç¨®'] == target_v)].copy()
    
    if isinstance(date_range, list) or isinstance(date_range, tuple):
        if len(date_range) == 2:
            start_date, end_date = date_range
            f_df = f_df[(f_df['æ—¥æœŸ'] >= start_date) & (f_df['æ—¥æœŸ'] <= end_date)]
        elif len(date_range) == 1:
            f_df = f_df[f_df['æ—¥æœŸ'] == date_range[0]]

    # æœå°‹æ¡†
    sc1, sc2 = st.columns(2)
    with sc1: s_sub = st.text_input("ğŸ” æœå°‹å°ä»£")
    with sc2: s_buy = st.text_input("ğŸ‘¤ æœå°‹è²·å®¶")

    if s_sub: f_df = f_df[f_df['å°ä»£'].str.contains(s_sub)]
    if s_buy: f_df = f_df[f_df['è²·å®¶'].str.contains(s_buy)]

    # --- é¡¯ç¤ºè¡¨æ ¼ ---
    display_cols = ["é¡¯ç¤ºæ—¥æœŸ", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_grade: display_cols.insert(1, "ç­‰ç´š")
    if show_total: 
        idx = display_cols.index("å–®åƒ¹") + 1
        display_cols.insert(idx, "ç¸½åƒ¹")
    if show_serial: display_cols.insert(0, "æµæ°´è™Ÿ")
    
    st.dataframe(f_df[display_cols].rename(columns={"é¡¯ç¤ºæ—¥æœŸ": "æ—¥æœŸ"}), use_container_width=True, height=450, hide_index=True)

    # --- çµ±è¨ˆè³‡è¨Šå€ ---
    st.divider()
    if not f_df.empty:
        t_pcs, t_kg, t_val = f_df['ä»¶æ•¸'].sum(), f_df['å…¬æ–¤'].sum(), f_df['ç¸½åƒ¹'].sum()
        avg_p = t_val / t_kg if t_kg > 0 else 0
        
        st.markdown(f"##### ğŸ“‰ {target_farm} ({target_v}) æ•¸æ“šæ‘˜è¦")
        m_cols = st.columns(6)
        metrics = [
            ("ç¸½ä»¶æ•¸", f"{int(t_pcs)} ä»¶"), ("ç¸½å…¬æ–¤", f"{int(t_kg)} kg"),
            ("æœ€é«˜åƒ¹", f"{f_df['å–®åƒ¹'].max()} å…ƒ"), ("æœ€ä½åƒ¹", f"{f_df['å–®åƒ¹'].min()} å…ƒ"),
            ("å¹³å‡å–®åƒ¹", f"{avg_p:.1f} å…ƒ"), ("å€é–“ç¸½åƒ¹", f"{int(t_val):,} å…ƒ")
        ]
        for i, (l, v) in enumerate(metrics):
            with m_cols[i]:
                st.markdown(f'<div style="background-color:#f0f2f6;padding:10px;border-radius:5px;text-align:center;">'
                            f'<p style="margin:0;font-size:12px;color:#555;">{l}</p>'
                            f'<p style="margin:0;font-size:16px;font-weight:bold;color:#111;">{v}</p></div>', unsafe_allow_html=True)
else:
    st.warning("ğŸ˜­ è®€å–å¤±æ•—ï¼Œè«‹ç¢ºèªè³‡æ–™æºã€‚")