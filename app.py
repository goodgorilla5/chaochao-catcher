import streamlit as st
import pandas as pd
import re
import requests
import concurrent.futures

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="è¾²æœƒè¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

# è¾²æœƒå®šç¾©ï¼šåªè¦å…§å®¹åŒ…å«é€™äº›ä»£ç¢¼ï¼Œå°±è‡ªå‹•æ­¸é¡
FARMER_MAP = {
    "ç‡•å·¢": "S00076",
    "å¤§ç¤¾": "S00250",
    "é˜¿è“®": "S00098",
    "é«˜æ¨¹": "T00493"
}

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹è‡³ Streamlit å¾Œå° Secrets è¨­å®š github_token")
    st.stop()

# --- æ ¸å¿ƒè§£æé‚è¼¯ ---
def process_logic(content):
    # æ”¹ç”¨æ›´å¼·å¤§çš„æ­£å‰‡åˆ†å‰²ï¼Œè™•ç†ä¸è¦å‰‡ç©ºæ ¼èˆ‡æ›è¡Œ
    # æˆ‘å€‘æ‰¾å°‹ç¬¦åˆæ•¸æ“šç‰¹å¾µçš„è¡Œï¼šåŒ…å« + è™Ÿä¸”åŒ…å«å“é …ä»£ç¢¼ F22
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    # å…ˆçµ±ä¸€å°‡å…§å®¹ä¸­çš„å¥‡æ€ªæ›è¡Œç¬¦è™•ç†æ‰ï¼Œä¸¦ä»¥ 4 å€‹ä»¥ä¸Šç©ºæ ¼æˆ–æ›è¡Œä¾†åˆæ­¥åˆ‡åˆ†ç­†æ•¸
    parts = re.split(r'\s{4,}|\n', content)
    
    for line in parts:
        line = line.strip()
        if "F22" in line and "+" in line:
            try:
                # 1. åˆ¤å®šè¾²æœƒ
                belong_to = "æœªçŸ¥"
                for name, code in FARMER_MAP.items():
                    if code in line:
                        belong_to = name
                        break
                if belong_to == "æœªçŸ¥": continue

                # 2. å®šä½å¸‚å ´ä»£ç¢¼ (Sæˆ–Té–‹é ­åŠ 5ä½æ•¸å­—)
                m_match = re.search(r"([S|T]\d{5})", line)
                if not m_match: continue
                m_pos = m_match.start()
                m_code = m_match.group(1)

                # 3. æå–å°ä»£èˆ‡ç­‰ç´š
                # ç­‰ç´šåœ¨å¸‚å ´ä»£ç¢¼å‰ 2 ä½
                level_code = line[m_pos-2]
                level = grade_map.get(level_code, level_code)
                # å°ä»£åœ¨å¸‚å ´ä»£ç¢¼å¾Œ 6 ä½é–‹å§‹çš„ 3 ç¢¼
                sub_id = line[m_pos+6:m_pos+9].strip()

                # 4. æå–æ—¥æœŸ (å°‹æ‰¾ 7 æˆ– 8 ä½æ•¸å­—ï¼Œé€šå¸¸åœ¨å¸‚å ´ä»£ç¢¼å‰é¢ä¸€æ®µè·é›¢)
                # é€™è£¡æ”¹ç”¨æ›´ç©©å®šçš„ç›¸å°å®šä½ï¼Œæ‰¾å¸‚å ´ä»£ç¢¼å·¦å´æœ€è¿‘çš„é•·æ•¸å­—
                date_search = re.findall(r"(\d{7,8})", line[:m_pos])
                raw_date_str = date_search[-1][:7] if date_search else "0000000"

                # 5. è™•ç†æµæ°´è™Ÿ (æ•´è¡Œæœ€å‰é¢åˆ°æ—¥æœŸå‰ï¼Œå»ç©ºæ ¼)
                serial = line[:line.find(raw_date_str)].replace(" ", "") if raw_date_str != "0000000" else "Unknown"

                # 6. æ•¸å€¼æå– (é€é + è™Ÿ)
                nums = line.split('+')
                pieces = int(nums[0][-3:].strip() or 0)
                weight = int(nums[1].strip() or 0)
                price_part = nums[2].strip().split(' ')[0]
                price = int(price_part[:-1] if price_part else 0)
                total_price = int(nums[3].strip() or 0)
                # è²·å®¶é€šå¸¸åœ¨æœ€å¾Œä¸€å€‹ + è™Ÿå¾Œ
                buyer = nums[-1].strip()[:4]

                rows.append({
                    "è¾²æœƒ": belong_to,
                    "æ—¥æœŸç·¨ç¢¼": raw_date_str,
                    "é¡¯ç¤ºæ—¥æœŸ": f"{raw_date_str[:3]}/{raw_date_str[3:5]}/{raw_date_str[5:7]}",
                    "æµæ°´è™Ÿ": serial, "ç­‰ç´š": level, "å°ä»£": sub_id, 
                    "ä»¶æ•¸": pieces, "å…¬æ–¤": weight, "å–®åƒ¹": price, 
                    "ç¸½åƒ¹": total_price, "è²·å®¶": buyer
                })
            except: continue
    return rows

@st.cache_data(ttl=60)
def fetch_all_github_data():
    all_rows = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    REPO_OWNER = "goodgorilla5"
    REPO_NAME = "chaochao-catcher"
    API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"
    
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        
        # ã€ä¿®æ”¹é»ã€‘ï¼šä¸é™åˆ¶æª”åé–‹é ­ï¼Œåªè¦æ˜¯ .SCP å°±æŠ“
        files = [f for f in r.json() if f['name'].lower().endswith('.scp')]
        
        def download_and_parse(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                # å˜—è©¦ Big5 è®€å–
                return process_logic(res.content.decode("big5", errors="ignore"))
            return []
            
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(download_and_parse, files))
        
        for r_list in results: all_rows.extend(r_list)
        df = pd.DataFrame(all_rows)
        if not df.empty:
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ", keep='first')
            df['date_obj'] = pd.to_datetime(df['æ—¥æœŸç·¨ç¢¼'].apply(lambda x: str(int(x[:3])+1911)+x[3:]), format='%Y%m%d')
            df = df.sort_values(by=["date_obj", "å–®åƒ¹"], ascending=[False, False])
        return df
    except: return pd.DataFrame()

# --- ä¸»ä»‹é¢ ---
st.title("ğŸ è¾²æœƒè¡Œæƒ…å¤§æ•¸æ“šåº« (èœœæ£—)")
df = fetch_all_github_data()

if not df.empty:
    target_farm = st.selectbox("ğŸ¥ é¸æ“‡è¾²æœƒ", options=["ç‡•å·¢", "å¤§ç¤¾", "é˜¿è“®", "é«˜æ¨¹"], index=0)
    
    min_d, max_d = df['date_obj'].min().date(), df['date_obj'].max().date()
    date_range = st.date_input("ğŸ“… é¸æ“‡æ—¥æœŸå€é–“", value=(max_d, max_d), min_value=min_d, max_value=max_d)
    
    sc1, sc2 = st.columns(2)
    with sc1: search_sub = st.text_input("ğŸ” æœå°‹å°ä»£")
    with sc2: search_buyer = st.text_input("ğŸ‘¤ æœå°‹è²·å®¶")

    st.sidebar.header("ğŸ¨ é¡¯ç¤ºè¨­å®š")
    show_level = st.sidebar.checkbox("é¡¯ç¤ºç­‰ç´š", value=False)
    show_total_p = st.sidebar.checkbox("é¡¯ç¤ºç¸½åƒ¹", value=False)

    # éæ¿¾
    f_df = df[df['è¾²æœƒ'] == target_farm].copy()
    if isinstance(date_range, tuple) and len(date_range) == 2:
        f_df = f_df[(f_df['date_obj'].dt.date >= date_range[0]) & (f_df['date_obj'].dt.date <= date_range[1])]
    if search_sub: f_df = f_df[f_df['å°ä»£'].str.contains(search_sub)]
    if search_buyer: f_df = f_df[f_df['è²·å®¶'].str.contains(search_buyer)]

    # è¡¨æ ¼é¡¯ç¤º
    display_cols = ["é¡¯ç¤ºæ—¥æœŸ", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_level: display_cols.insert(1, "ç­‰ç´š")
    if show_total_p: display_cols.insert(display_cols.index("å–®åƒ¹")+1, "ç¸½åƒ¹")
    
    st.dataframe(f_df[display_cols].rename(columns={"é¡¯ç¤ºæ—¥æœŸ": "æ—¥æœŸ"}), 
                 use_container_width=True, height=450, hide_index=True)

    # åº•éƒ¨çµ±è¨ˆ
    st.divider()
    if not f_df.empty:
        t_pcs, t_kg, t_val = f_df['ä»¶æ•¸'].sum(), f_df['å…¬æ–¤'].sum(), f_df['ç¸½åƒ¹'].sum()
        avg_p = t_val / t_kg if t_kg > 0 else 0
        st.markdown(f"##### ğŸ“‰ {target_farm}å€ - æ•¸æ“šæ‘˜è¦")
        cols = st.columns(6)
        m_list = [("ç¸½ä»¶æ•¸", f"{t_pcs} ä»¶"), ("ç¸½å…¬æ–¤", f"{t_kg} kg"), ("æœ€é«˜åƒ¹", f"{f_df['å–®åƒ¹'].max()} å…ƒ"),
                  ("æœ€ä½åƒ¹", f"{f_df['å–®åƒ¹'].min()} å…ƒ"), ("å¹³å‡å–®åƒ¹", f"{avg_p:.2f} å…ƒ"), ("å€é–“ç¸½åƒ¹", f"{t_val:,} å…ƒ")]
        for i, (l, v) in enumerate(m_list):
            with cols[i]:
                st.markdown(f'<div style="background-color:#f0f2f6;padding:10px;border-radius:5px;text-align:center;">'
                            f'<p style="margin:0;font-size:12px;color:#555;">{l}</p>'
                            f'<p style="margin:0;font-size:15px;font-weight:bold;color:#111;">{v}</p></div>', unsafe_allow_html=True)
    else:
        st.info(f"ğŸ’¡ ç›®å‰ {target_farm} ç„¡ç›¸é—œ F22 æˆäº¤è³‡æ–™ã€‚")
else:
    st.warning("ğŸ˜­ å€‰åº«ä¸­ç›®å‰æ²’æœ‰ä»»ä½• .SCP æª”æ¡ˆæˆ–å…§å®¹è§£æå¤±æ•—ã€‚")