import streamlit as st
import pandas as pd
import re
import requests
import time
from datetime import datetime
import concurrent.futures

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

# --- è¨­å®šå€ ---
REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

# è®€å–å‰›å‰›æ¸¬è©¦æˆåŠŸçš„ Token
try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ æ‰¾ä¸åˆ° github_tokenï¼è«‹è‡³ Streamlit å¾Œå° Secrets è¨­å®šã€‚")
    st.stop()

# --- æ ¸å¿ƒè§£æé‚è¼¯ ---
def parse_scp_content(content):
    raw_lines = content.split('    ')
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    for line in raw_lines:
        if "F22" in line and "S00076" in line:
            try:
                s_pos = line.find("S00076")
                # æŠ“å– 7 ä½æ—¥æœŸ (æ°‘åœ‹æ—¥æœŸ)
                date_match = re.search(r"(\d{7})", line[max(0, s_pos-20):s_pos])
                if date_match:
                    real_date_str = date_match.group(1)
                    formatted_date = f"{real_date_str[:3]}/{real_date_str[3:5]}/{real_date_str[5:7]}"
                    
                    # ä½¿ç”¨è¶…é•·æµæ°´è™Ÿä½œç‚ºå”¯ä¸€ ID
                    date_idx = line.find(real_date_str)
                    serial = line[:date_idx+15].strip().replace(" ", "")

                    level = grade_map.get(line[s_pos-2], line[s_pos-2])
                    sub_id = line[s_pos+6:s_pos+9]
                    
                    nums = line.split('+')
                    pieces = int(nums[0][-3:].strip() or 0)
                    weight = int(nums[1].strip() or 0)
                    price_raw = nums[2].strip().split(' ')[0]
                    price = int(re.sub(r'\D', '', price_raw) if price_raw else 0)
                    buyer = nums[5].strip()[:4]

                    rows.append({
                        "æµæ°´è™Ÿ": serial, "æ—¥æœŸ": formatted_date, "ç­‰ç´š": level, 
                        "å°ä»£": sub_id, "ä»¶æ•¸": pieces, "å…¬æ–¤": weight, 
                        "å–®åƒ¹": price, "è²·å®¶": buyer
                    })
            except: continue
    return rows

# --- æŠ“å– GitHub æ‰€æœ‰æª”æ¡ˆ ---
@st.cache_data(ttl=300)
def fetch_all_data():
    all_data = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        
        file_list = [f for f in r.json() if f['name'].upper().endswith(('.SCP', '.TXT'))]
        if not file_list: return pd.DataFrame()

        def download_and_parse(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                # åˆ¤æ–·æ˜¯å¦ç‚ºçœŸæ­£çš„è¡Œæƒ…æª” (æ’é™¤ä¹‹å‰ä¸‹è¼‰éŒ¯çš„ HTML)
                text_content = res.content.decode("big5", errors="ignore")
                if "<!DOCTYPE" in text_content or "<html>" in text_content:
                    return []
                return parse_scp_content(text_content)
            return []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(download_and_parse, file_list))
        
        for r in results: all_data.extend(r)
        
        df = pd.DataFrame(all_data)
        if not df.empty:
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ", keep="first")
            df = df.sort_values(by=["æ—¥æœŸ", "å–®åƒ¹"], ascending=[False, False])
        return df
    except:
        return pd.DataFrame()

# --- ç¶²é ä»‹é¢ ---
st.title("ğŸ“Š ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

with st.spinner('é€£ç·šæˆåŠŸï¼æ­£åœ¨åˆä½µé›²ç«¯è³‡æ–™...'):
    df_all = fetch_all_data()

if not df_all.empty:
    st.sidebar.header("ğŸ› ï¸ æ•¸æ“šç¯©é¸")
    all_dates = sorted(df_all['æ—¥æœŸ'].unique(), reverse=True)
    selected_dates = st.sidebar.multiselect("ğŸ“… é¸æ“‡æ—¥æœŸ (ä¸é¸å‰‡é¡¯ç¤ºå…¨éƒ¨)", all_dates)
    search_sub = st.sidebar.text_input("ğŸ” æœå°‹å°ä»£")
    
    filtered_df = df_all.copy()
    if selected_dates:
        filtered_df = filtered_df[filtered_df['æ—¥æœŸ'].isin(selected_dates)]
    if search_sub:
        filtered_df = filtered_df[filtered_df['å°ä»£'].str.contains(search_sub)]

    # é¡¯ç¤ºçµ±è¨ˆæŒ‡æ¨™
    c1, c2, c3 = st.columns(3)
    c1.metric("ä»¶æ•¸ç¸½è¨ˆ", f"{filtered_df['ä»¶æ•¸'].sum()} ä»¶")
    c2.metric("å€é–“æœ€é«˜å–®åƒ¹", f"{filtered_df['å–®åƒ¹'].max()} å…ƒ")
    c3.metric("è³‡æ–™ç­†æ•¸", f"{len(filtered_df)} ç­†")

    st.divider()
    st.dataframe(filtered_df, use_container_width=True, height=600)
else:
    st.warning("âš ï¸ é›–ç„¶é€£ç·šæ­£å¸¸ï¼Œä½† GitHub å…§ä¼¼ä¹æ²’æœ‰æ­£ç¢ºçš„è¡Œæƒ…æª”æ¡ˆã€‚")
    st.info("è«‹æª¢æŸ¥ GitHub å…§çš„ .SCP æª”æ¡ˆå…§å®¹æ˜¯å¦æ­£ç¢ºï¼ˆä¸å¯åŒ…å« HTML æ¨™ç±¤ï¼‰ã€‚")