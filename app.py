import streamlit as st
import pandas as pd
import re
import requests
import concurrent.futures
from datetime import datetime

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

# --- GitHub è¨­å®šå€ ---
REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹è‡³ Streamlit å¾Œå° Secrets è¨­å®š github_token")
    st.stop()

# --- æ ¸å¿ƒè§£æé‚è¼¯ ---
def process_logic(content):
    raw_lines = content.split('    ')
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    for line in raw_lines:
        if "F22" in line and "S00076" in line:
            try:
                date_match = re.search(r"(\d{7,8}1)\s+\d{2}S00076", line)
                if date_match:
                    date_pos = date_match.start()
                    raw_date_str = date_match.group(1)[:7]
                    serial = line[:date_pos].strip().replace(" ", "")
                    
                    remaining = line[date_pos:]
                    s_pos = remaining.find("S00076")
                    level = grade_map.get(remaining[s_pos-2], remaining[s_pos-2])
                    sub_id = remaining[s_pos+6:s_pos+9]
                    
                    nums = line.split('+')
                    pieces = int(nums[0][-3:].replace(" ", "") or 0)
                    weight = int(nums[1].replace(" ", "") or 0)
                    price_raw = nums[2].strip().split(' ')[0]
                    price = int(price_raw[:-1] if price_raw else 0)
                    total_price = int(nums[3].replace(" ", "") or 0)
                    buyer = nums[5].strip()[:4] if len(nums) > 5 else ""

                    rows.append({
                        "æ—¥æœŸç·¨ç¢¼": raw_date_str,
                        "é¡¯ç¤ºæ—¥æœŸ": f"{raw_date_str[:3]}/{raw_date_str[3:5]}/{raw_date_str[5:7]}",
                        "æµæ°´è™Ÿ": serial, 
                        "ç­‰ç´š": level, 
                        "å°ä»£": sub_id, 
                        "ä»¶æ•¸": pieces, 
                        "å…¬æ–¤": weight, 
                        "å–®åƒ¹": price, 
                        "ç¸½åƒ¹": total_price,
                        "è²·å®¶": buyer
                    })
            except: 
                continue
    return rows

@st.cache_data(ttl=60)
def fetch_all_github_data():
    all_rows = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        files = [f for f in r.json() if f['name'].upper().endswith('.SCP')]
        
        def download_and_parse(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                text = res.content.decode("big5", errors="ignore")
                return process_logic(text)
            return []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(download_and_parse, files))
        
        for r_list in results: all_rows.extend(r_list)
        df = pd.DataFrame(all_rows)
        if not df.empty:
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ", keep='first')
            df['date_obj'] = pd.to_datetime(df['æ—¥æœŸç·¨ç¢¼'].apply(lambda x: str(int(x[:3])+1911)+x[3:]), format='%Y%m%d')
            df = df.sort_values(by=["date_obj", "å–®åƒ¹"], ascending=[False, False])
        return df
    except:
        return pd.DataFrame()

# --- è®€å–è³‡æ–™ ---
df = fetch_all_github_data()

st.title("ğŸ ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

if not df.empty:
    # --- 1. è¡¨æ ¼ä¸Šæ–¹ï¼šæŸ¥è©¢æ§åˆ¶å€ ---
    min_d, max_d = df['date_obj'].min().date(), df['date_obj'].max().date()
    
    # æ‰‹æ©Ÿæ’ç‰ˆå„ªåŒ–ï¼šæ§åˆ¶å€æ’æˆå…©æ¬„
    ctrl_c1, ctrl_c2 = st.columns([2, 1])
    with ctrl_c1:
        date_range = st.date_input("ğŸ“… é¸æ“‡æŸ¥è©¢å€é–“", value=(max_d, max_d), min_value=min_d, max_value=max_d)
    with ctrl_c2:
        search_sub = st.text_input("ğŸ” æœå°‹å°ä»£ (å¦‚ 627)", placeholder="è¼¸å…¥ä»£è™Ÿ")

    # å´é‚Šæ¬„åƒ…ä¿ç•™ã€Œé¡¯ç¤ºè¨­å®šã€
    st.sidebar.header("ğŸ¨ é¡¯ç¤ºè¨­å®š")
    show_level = st.sidebar.checkbox("é¡¯ç¤ºç­‰ç´š", value=False)
    show_total_p = st.sidebar.checkbox("é¡¯ç¤ºç¸½åƒ¹", value=False)
    show_serial = st.sidebar.checkbox("é¡¯ç¤ºåŸå§‹æµæ°´è™Ÿ", value=False)

    # éæ¿¾é‚è¼¯
    f_df = df.copy()
    if isinstance(date_range, tuple) and len(date_range) == 2:
        f_df = f_df[(f_df['date_obj'].dt.date >= date_range[0]) & (f_df['date_obj'].dt.date <= date_range[1])]
    if search_sub:
        f_df = f_df[f_df['å°ä»£'].str.contains(search_sub)]

    # --- 2. è¡Œæƒ…è¡¨æ ¼é¡¯ç¤º ---
    display_cols = ["é¡¯ç¤ºæ—¥æœŸ", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_level: display_cols.insert(1, "ç­‰ç´š")
    if show_total_p:
        idx = display_cols.index("å–®åƒ¹") + 1
        display_cols.insert(idx, "ç¸½åƒ¹")
    if show_serial: display_cols.insert(0, "æµæ°´è™Ÿ")
    
    st.dataframe(
        f_df[display_cols].rename(columns={"é¡¯ç¤ºæ—¥æœŸ": "æ—¥æœŸ"}), 
        use_container_width=True, 
        height=500, # ç¨å¾®èª¿ä½é«˜åº¦ï¼Œè®“ä¸‹æ–¹çµ±è¨ˆè³‡è¨Šéœ²å‡º
        column_config={
            "å–®åƒ¹": st.column_config.NumberColumn(format="%d"),
            "ç¸½åƒ¹": st.column_config.NumberColumn(format="%d")
        }
    )

    # --- 3. è¡¨æ ¼ä¸‹æ–¹ï¼šçµ±è¨ˆè³‡è¨Šå€ ---
    st.divider()
    t_pcs, t_kg, t_val = f_df['ä»¶æ•¸'].sum(), f_df['å…¬æ–¤'].sum(), f_df['ç¸½åƒ¹'].sum()
    avg_p = t_val / t_kg if t_kg > 0 else 0

    # ä½¿ç”¨è¼ƒå°çš„ columns å­—é«”
    st.markdown("##### ğŸ“‰ å€é–“æ•¸æ“šæ‘˜è¦")
    m1, m2, m3, m4, m5, m6 = st.columns(6)
    m1.metric("ç¸½ä»¶æ•¸", f"{t_pcs} ä»¶")
    m2.metric("ç¸½å…¬æ–¤", f"{t_kg} kg")
    m3.metric("æœ€é«˜åƒ¹", f"{f_df['å–®åƒ¹'].max()} å…ƒ")
    m4.metric("æœ€ä½åƒ¹", f"{f_df['å–®åƒ¹'].min()} å…ƒ")
    m5.metric("å¹³å‡å–®åƒ¹", f"{avg_p:.2f} å…ƒ")
    m6.metric("å€é–“ç¸½åƒ¹", f"{t_val:,} å…ƒ")

else:
    st.warning("ğŸ˜­ ç›®å‰é›²ç«¯å€‰åº«ä¸­æ²’æœ‰å¯è®€å–çš„è³‡æ–™ã€‚")