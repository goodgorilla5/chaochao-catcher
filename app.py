import streamlit as st
import pandas as pd
import re
import requests
import concurrent.futures

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹æª¢æŸ¥ Streamlit Secrets ä¸­çš„ github_token è¨­å®š")
    st.stop()

# --- æ ¸å¿ƒè§£æï¼šæ‡‰å°ç„¡æ›è¡Œé•·å­—ä¸²æ ¼å¼ ---
def parse_scp_content(content):
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    # ä½¿ç”¨ã€Œå››å€‹ç©ºæ ¼ã€ä½œç‚ºæ¯ä¸€ç­†äº¤æ˜“ç´€éŒ„çš„åˆ‡å‰²é»
    entries = content.split('    ')
    
    for entry in entries:
        # åªæŠ“å–åŒ…å«å°åŒ—å¸‚å ´ (S00076) çš„ç´€éŒ„
        if "S00076" in entry:
            try:
                # å®šä½å¸‚å ´ä»£ç¢¼ S00076
                s_pos = entry.find("S00076")
                
                # 1. æŠ“å–æ—¥æœŸï¼šS00076 å¾€å‰æ•¸ç¬¬ 9 åˆ°ç¬¬ 2 ä½ (ä¾‹å¦‚ 1150210)
                date_part = entry[s_pos-9 : s_pos-2]
                if not date_part.isdigit() or len(date_part) != 7:
                    continue
                
                formatted_date = f"{date_part[:3]}/{date_part[3:5]}/{date_part[5:7]}"
                
                # 2. æµæ°´è™Ÿï¼šå–è©²ç­†ç´€éŒ„çš„å‰ 30 ç¢¼ï¼ˆåŒ…å«æ—¥æœŸé›œè¨Šä¹Ÿæ²’é—œä¿‚ï¼Œåªè¦å”¯ä¸€å³å¯ï¼‰
                serial = entry[:30].strip()

                # 3. ç­‰ç´šèˆ‡å°ä»£
                # ç­‰ç´šåœ¨ S00076 å¾€å‰ 2 æ ¼
                level_code = entry[s_pos-2]
                level = grade_map.get(level_code, level_code)
                # å°ä»£åœ¨ S00076 å¾€å¾Œ 6 æ ¼é–‹å§‹çš„ 3 ä½
                sub_id = entry[s_pos+6:s_pos+9]
                
                # 4. è§£ææ•¸æ“šå€ (ä»¶æ•¸+é‡é‡+å–®åƒ¹)
                # æ ¼å¼: 003+00018+01400+...
                parts = entry.split('+')
                if len(parts) >= 3:
                    # ä»¶æ•¸ï¼šç¬¬ä¸€å€‹åŠ è™Ÿå‰çš„æœ€å¾Œ 3 ä½
                    pieces = int(re.sub(r'\D', '', parts[0][-3:]))
                    # å…¬æ–¤ï¼šç¬¬ä¸€å€‹èˆ‡ç¬¬äºŒå€‹åŠ è™Ÿä¹‹é–“
                    weight = int(re.sub(r'\D', '', parts[1]))
                    # å–®åƒ¹ï¼šç¬¬äºŒå€‹åŠ è™Ÿå¾Œé¢çš„æ•¸å­—æ®µ
                    price_str = parts[2].strip().split(' ')[0]
                    price = int(re.sub(r'\D', '', price_str))
                    # è²·å®¶ï¼šæœ€å¾Œä¸€å€‹åŠ è™Ÿå¾Œé¢çš„å…§å®¹ (é€šå¸¸æ˜¯å¾Œå››ç¢¼)
                    buyer = parts[-1].strip()[:4]

                    rows.append({
                        "æµæ°´è™Ÿ": serial, 
                        "æ—¥æœŸ": formatted_date, 
                        "ç­‰ç´š": level, 
                        "å°ä»£": sub_id, 
                        "ä»¶æ•¸": pieces, 
                        "å…¬æ–¤": weight, 
                        "å–®åƒ¹": price, 
                        "è²·å®¶": buyer
                    })
            except Exception:
                continue
    return rows

@st.cache_data(ttl=300)
def fetch_all_data():
    all_data = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        
        # éæ¿¾å‡ºæ‰€æœ‰ SCP æª”æ¡ˆ
        files = [f for f in r.json() if f['name'].upper().endswith('.SCP')]
        
        def process_file(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                # å®˜ç¶²åŸå§‹æª”æ¡ˆç·¨ç¢¼æ˜¯ big5
                text = res.content.decode("big5", errors="ignore")
                # æ’é™¤ HTML æ®˜ç•™æª”
                if "<!DOCTYPE" in text or "<html>" in text:
                    return []
                return parse_scp_content(text)
            return []

        # ä½¿ç”¨ä¸¦è¡Œä¸‹è¼‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(process_file, files))
        
        for r_list in results:
            all_data.extend(r_list)
            
        df = pd.DataFrame(all_data)
        if not df.empty:
            # ä¾æ“šæµæ°´è™Ÿæ’é™¤é‡è¤‡
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ")
            # æ’åºï¼šæ—¥æœŸç”±æ–°åˆ°èˆŠï¼Œåƒ¹æ ¼ç”±é«˜åˆ°ä½
            df = df.sort_values(by=["æ—¥æœŸ", "å–®åƒ¹"], ascending=[False, False])
        return df
    except Exception as e:
        return pd.DataFrame()

# --- ç¶²é ä¸»ç•«é¢ ---
st.title("ğŸ“Š ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

# ç²å–è³‡æ–™
df = fetch_all_data()

if not df.empty:
    st.sidebar.header("ğŸ› ï¸ æ•¸æ“šç¯©é¸")
    
    # 1. æ—¥æœŸå¤šé¸
    all_dates = sorted(df['æ—¥æœŸ'].unique(), reverse=True)
    sel_dates = st.sidebar.multiselect("ğŸ“… é¸æ“‡æ—¥æœŸ (ä¸é¸å‰‡é¡¯ç¤ºå…¨éƒ¨)", all_dates)
    
    # 2. å°ä»£æœå°‹
    search_sub = st.sidebar.text_input("ğŸ” æœå°‹å°ä»£ (å¦‚ 627)")
    
    # 3. æµæ°´è™Ÿé–‹é—œ
    show_serial = st.sidebar.checkbox("é¡¯ç¤ºåŸå§‹æµæ°´è™Ÿ (é è¨­éš±è—)", value=False)

    # éæ¿¾é‚è¼¯
    f_df = df.copy()
    if sel_dates:
        f_df = f_df[f_df['æ—¥æœŸ'].isin(sel_dates)]
    if search_sub:
        f_df = f_df[f_df['å°ä»£'].str.contains(search_sub)]

    # é ‚éƒ¨çµ±è¨ˆæŒ‡æ¨™
    c1, c2, c3 = st.columns(3)
    c1.metric("ç¸½ä»¶æ•¸", f"{f_df['ä»¶æ•¸'].sum()} ä»¶")
    c2.metric("æœ€é«˜å–®åƒ¹", f"{f_df['å–®åƒ¹'].max()} å…ƒ")
    c3.metric("è³‡æ–™ç­†æ•¸", f"{len(f_df)} ç­†")

    st.divider()
    
    # æ¬„ä½é¡¯ç¤ºè¨­å®š
    cols = ["æ—¥æœŸ", "ç­‰ç´š", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_serial:
        cols.insert(0, "æµæ°´è™Ÿ")
    
    st.dataframe(f_df[cols], use_container_width=True, height=600)
else:
    st.warning("âš ï¸ ç›®å‰é›²ç«¯å°šæœªæœ‰å¯æ­£ç¢ºè§£æçš„è³‡æ–™ã€‚")
    st.info("è«‹ç¢ºèª GitHub ä¸­çš„ .SCP æª”æ¡ˆæ˜¯ç”¨æ›¸ç±¤æ‰‹å‹•ä¸‹è¼‰çš„ç‰ˆæœ¬ã€‚")