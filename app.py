import streamlit as st
import pandas as pd
import re
import requests
import concurrent.futures
from datetime import datetime

st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

# --- è¨­å®šå€ ---
REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹æª¢æŸ¥ Streamlit Secrets ä¸­çš„ github_token")
    st.stop()

# --- æ ¸å¿ƒè§£æé‚è¼¯ (å®Œå…¨æ²¿ç”¨æ‚¨æœ€ä¿¡ä»»çš„é‚è¼¯) ---
def process_logic(content):
    raw_lines = content.split('    ')
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    for line in raw_lines:
        if "F22" in line and "S00076" in line:
            try:
                # æ‚¨çš„åŸå§‹ç²¾æº–å®šä½é‚è¼¯
                date_match = re.search(r"(\d{7,8}1)\s+\d{2}S00076", line)
                if date_match:
                    date_pos = date_match.start()
                    # æŠ“å–æ—¥æœŸå­—ä¸²ä¸¦æ ¼å¼åŒ–
                    raw_date_str = date_match.group(1)[:7]
                    formatted_date = f"{raw_date_str[:3]}/{raw_date_str[3:5]}/{raw_date_str[5:7]}"
                    
                    # åˆä½µæµæ°´è™Ÿç©ºæ ¼
                    serial = line[:date_pos].strip().replace(" ", "")
                    
                    remaining = line[date_pos:]
                    s_pos = remaining.find("S00076")
                    level = grade_map.get(remaining[s_pos-2], remaining[s_pos-2])
                    sub_id = remaining[s_pos+6:s_pos+9]
                    
                    nums = line.split('+')
                    pieces = int(nums[0][-3:].replace(" ", "") or 0)
                    weight = int(nums[1].replace(" ", "") or 0)
                    price_raw = nums[2].strip().split(' ')[0]
                    # è™•ç†å–®åƒ¹ï¼ˆç§»é™¤æœ€å¾Œä¸€ä½å¯èƒ½æ˜¯æ ¡é©—ç¢¼æˆ–ç©ºæ ¼çš„å­—å…ƒï¼‰
                    price = int(price_raw[:-1] if price_raw else 0)
                    buyer = nums[5].strip()[:4] if len(nums) > 5 else ""

                    rows.append({
                        "æ—¥æœŸ": formatted_date,
                        "æµæ°´è™Ÿ": serial, 
                        "ç­‰ç´š": level, 
                        "å°ä»£": sub_id, 
                        "ä»¶æ•¸": pieces, 
                        "å…¬æ–¤": weight, 
                        "å–®åƒ¹": price, 
                        "è²·å®¶": buyer
                    })
            except: continue
    return rows

# --- è‡ªå‹•è®€å– GitHub æ‰€æœ‰ SCP æª”æ¡ˆ ---
@st.cache_data(ttl=300)
def fetch_all_github_data():
    all_data = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        
        # æ‰¾å‡ºæ‰€æœ‰ SCP çµå°¾çš„æª”æ¡ˆ
        files = [f for f in r.json() if f['name'].upper().endswith('.SCP')]
        
        def download_and_parse(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                content = res.content.decode("big5", errors="ignore")
                return process_logic(content)
            return []

        # ä¸¦è¡Œä¸‹è¼‰ä»¥æé«˜é€Ÿåº¦
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(download_and_parse, files))
        
        for r_list in results:
            all_data.extend(r_list)
            
        df = pd.DataFrame(all_data)
        if not df.empty:
            # --- é—œéµåŠŸèƒ½ï¼šå‰”é™¤é‡è¤‡æµæ°´è™Ÿ ---
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ", keep='first')
            # è½‰æ›æ—¥æœŸæ ¼å¼ä»¥ä¾¿æ’åº
            df['date_obj'] = pd.to_datetime(df['æ—¥æœŸ'].apply(lambda x: str(int(x.split('/')[0])+1911)+x[3:].replace('/','')), format='%Y%m%d')
            df = df.sort_values(by="date_obj", ascending=False)
        return df
    except:
        return pd.DataFrame()

# --- ä¸»ç•«é¢ ---
st.title("ğŸ“Š ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

df = fetch_all_github_data()

if not df.empty:
    st.sidebar.header("ğŸ—“ï¸ æŸ¥è©¢ç¯„åœè¨­å®š")
    
    # --- æ–°å¢åŠŸèƒ½ï¼šæ—¥æœŸå€é–“æœå°‹ ---
    min_date = df['date_obj'].min()
    max_date = df['date_obj'].max()
    
    date_range = st.sidebar.date_input(
        "é¸æ“‡è¡Œæƒ…æ—¥æœŸå€é–“",
        value=(min_date, max_date),
        min_value=min_date,
        max_value=max_date
    )

    st.sidebar.divider()
    search_sub = st.sidebar.text_input("ğŸ” æœå°‹å°ä»£ (å¦‚ 627)")
    show_serial = st.sidebar.checkbox("é¡¯ç¤ºåŸå§‹æµæ°´è™Ÿ", value=False)

    # éæ¿¾é‚è¼¯
    if len(date_range) == 2:
        start_date, end_date = date_range
        mask = (df['date_obj'].dt.date >= start_date) & (df['date_obj'].dt.date <= end_date)
        f_df = df.loc[mask].copy()
    else:
        f_df = df.copy()

    if search_sub:
        f_df = f_df[f_df['å°ä»£'].str.contains(search_sub)]

    # çµ±è¨ˆè³‡è¨Š
    c1, c2, c3 = st.columns(3)
    c1.metric("æ‰€é¸å€é–“ç¸½ä»¶æ•¸", f"{f_df['ä»¶æ•¸'].sum()} ä»¶")
    c2.metric("å€é–“æœ€é«˜å–®åƒ¹", f"{f_df['å–®åƒ¹'].max()} å…ƒ")
    c3.metric("è³‡æ–™ç­†æ•¸ (å·²å»é‡)", f"{len(f_df)} ç­†")

    # é¡¯ç¤ºè¡¨æ ¼
    display_cols = ["æ—¥æœŸ", "ç­‰ç´š", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_serial:
        display_cols.insert(1, "æµæ°´è™Ÿ")
    
    st.dataframe(f_df[display_cols], use_container_width=True, height=600)

else:
    st.warning("ğŸ˜­ ç›®å‰é›²ç«¯å€‰åº«ä¸­æ²’æœ‰å¯è®€å–çš„ SCP æª”æ¡ˆã€‚")
    st.info("è«‹ç¢ºèª GitHub å€‰åº« goodgorilla5/chaochao-catcher ä¸­å·²æœ‰ä¸Šå‚³ SCP æª”ã€‚")