import streamlit as st
import pandas as pd
import re
import requests
import concurrent.futures

st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹è‡³ Streamlit å¾Œå° Secrets è¨­å®š github_token")
    st.stop()

def parse_scp_content(content):
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    # æ‰¾åˆ°æ‰€æœ‰ S00076 çš„èµ·å§‹ä½ç½®
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å°‹æ‰¾æ‰€æœ‰å¸‚å ´ä»£ç¢¼çš„åº§æ¨™
    for match in re.finditer(r'S00076', content):
        try:
            s_idx = match.start()
            
            # --- 1. å®šä½æ—¥æœŸ ---
            # æ ¹æ“š A11150210... 11502101 11S00076 æ ¼å¼
            # æ—¥æœŸ 1150210 å°±åœ¨ S00076 å¾€å‰æ•¸ç¬¬ 11 åˆ°ç¬¬ 5 å€‹å­—å…ƒçš„ä½ç½®
            raw_date = content[s_idx-11 : s_idx-4] 
            if not raw_date.isdigit(): continue
            formatted_date = f"{raw_date[:3]}/{raw_date[3:5]}/{raw_date[5:7]}"
            
            # --- 2. æŠ“å–æµæ°´è™Ÿ ---
            # æµæ°´è™Ÿå°±åœ¨æ—¥æœŸå€æ®µå†å¾€å‰å¤§ç´„ 30~40 å€‹å­—å…ƒçš„ä½ç½®
            # æˆ‘å€‘ç›´æ¥æŠ“å– S00076 å¾€å‰ 60 å€‹å­—å…ƒç›´åˆ°æ—¥æœŸå€æ®µå‰
            # é€™è£¡æˆ‘å€‘ã€Œåˆä½µæ‰€æœ‰ç©ºç™½ã€ä¾†è™•ç†ä½ æåˆ°çš„æµæ°´è™Ÿæ–·é–‹å•é¡Œ
            raw_serial_area = content[max(0, s_idx-60) : s_idx-11].strip()
            full_serial = re.sub(r'\s+', '', raw_serial_area) # å¼·åˆ¶åˆä½µæ‰€æœ‰ç©ºæ ¼
            
            # --- 3. æŠ“å–å…¶ä»–æ¬„ä½ (ç›¸å° S00076) ---
            level_code = content[s_idx-2]
            level = grade_map.get(level_code, level_code)
            sub_id = content[s_idx+6 : s_idx+9]
            
            # --- 4. è§£ææ•¸æ“šå€ (ä»¶æ•¸+é‡é‡+å–®åƒ¹) ---
            # å¾ S00076 å¾€å¾Œæ‰¾æœ€è¿‘çš„ä¸€ä¸²æ•¸æ“š (+ è™Ÿçµæ§‹)
            data_area = content[s_idx+10 : s_idx+80].split('    ')[0] # æŠ“åˆ°ä¸‹ä¸€ç­†å‰çš„å€æ®µ
            nums = data_area.split('+')
            
            if len(nums) >= 3:
                pieces = int(re.sub(r'\D', '', nums[0][-3:]))
                weight = int(re.sub(r'\D', '', nums[1]))
                price_val = nums[2].strip().split(' ')[0]
                price = int(re.sub(r'\D', '', price_val))
                buyer = nums[-1].strip()[:4]

                rows.append({
                    "æµæ°´è™Ÿ": full_serial,
                    "æ—¥æœŸ": formatted_date,
                    "ç­‰ç´š": level,
                    "å°ä»£": sub_id,
                    "ä»¶æ•¸": pieces,
                    "å…¬æ–¤": weight,
                    "å–®åƒ¹": price,
                    "è²·å®¶": buyer
                })
        except:
            continue
    return rows

@st.cache_data(ttl=300)
def fetch_all_data():
    all_data = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        files = [f for f in r.json() if f['name'].upper().endswith('.SCP')]
        
        def process_file(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                text = res.content.decode("big5", errors="ignore")
                return parse_scp_content(text)
            return []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(process_file, files))
        
        for r_list in results: all_data.extend(r_list)
        df = pd.DataFrame(all_data)
        if not df.empty:
            # ã€é—œéµã€‘ä»¥åˆä½µå¾Œçš„å®Œæ•´æµæ°´è™Ÿå»é‡
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ", keep='first')
            df = df.sort_values(by=["æ—¥æœŸ", "å–®åƒ¹"], ascending=[False, False])
        return df
    except: return pd.DataFrame()

# --- UI ä»‹é¢ ---
st.title("ğŸ“Š ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

df = fetch_all_data()

if not df.empty:
    st.sidebar.header("ğŸ› ï¸ æ•¸æ“šç¯©é¸")
    all_dates = sorted(df['æ—¥æœŸ'].unique(), reverse=True)
    sel_dates = st.sidebar.multiselect("ğŸ“… é¸æ“‡æ—¥æœŸ", all_dates)
    search_sub = st.sidebar.text_input("ğŸ” æœå°‹å°ä»£")
    show_serial = st.sidebar.checkbox("é¡¯ç¤ºåˆä½µå¾Œçš„æµæ°´è™Ÿ", value=False)

    f_df = df.copy()
    if sel_dates: f_df = f_df[f_df['æ—¥æœŸ'].isin(sel_dates)]
    if search_sub: f_df = f_df[f_df['å°ä»£'].str.contains(search_sub)]

    c1, c2, c3 = st.columns(3)
    c1.metric("ä»¶æ•¸ç¸½è¨ˆ", f"{f_df['ä»¶æ•¸'].sum()} ä»¶")
    c2.metric("æœ€é«˜å–®åƒ¹", f"{f_df['å–®åƒ¹'].max()} å…ƒ")
    c3.metric("è³‡æ–™ç­†æ•¸", f"{len(f_df)} ç­†")

    cols = ["æ—¥æœŸ", "ç­‰ç´š", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_serial: cols.insert(0, "æµæ°´è™Ÿ")
    st.dataframe(f_df[cols], use_container_width=True, height=600)
else:
    st.warning("âš ï¸ æƒæå®Œæˆï¼Œä½†ç›®å‰çš„æª”æ¡ˆæ ¼å¼ç„¡æ³•æå–è³‡æ–™ã€‚")