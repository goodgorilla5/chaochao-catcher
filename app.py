import streamlit as st
import pandas as pd
import re
import requests
import concurrent.futures

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹è‡³ Streamlit Secrets è¨­å®š github_token")
    st.stop()

# --- æ ¸å¿ƒè§£æï¼šåˆä½µæµæ°´è™Ÿä¸¦æŠ“å–è³‡æ–™ ---
def parse_scp_content(content):
    # 1. å…ˆæŠŠæ•´ä»½å…§å®¹çš„å¤šé¤˜ç©ºç™½ç¸®æ¸›ï¼Œä½†ä¿ç•™æ¬„ä½é–“çš„å€éš”ç‰¹å¾µ
    # é‡å°æµæ°´è™Ÿè¢«æ–·é–‹çš„å•é¡Œï¼Œæˆ‘å€‘å°‹æ‰¾åƒ "A11" æˆ– "A21" é–‹é ­çš„ç‰¹å¾µ
    # é€™è£¡æ¡ç”¨æ›´ç©©å¥çš„æ–¹æ³•ï¼šå°‡å…§å®¹ä¾ç…§ã€ŒçœŸæ­£ã€çš„é–“éš”ï¼ˆé€šå¸¸æ˜¯4å€‹ä»¥ä¸Šç©ºæ ¼ï¼‰åˆ‡é–‹
    raw_entries = re.split(r'\s{4,}', content)
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    for entry in raw_entries:
        if "S00076" in entry and "F22" in entry:
            try:
                # åˆä½µå¯èƒ½è¢«æ–·é–‹çš„æµæ°´è™Ÿéƒ¨åˆ†ï¼š
                # å‡è¨­æ ¼å¼æ˜¯ [æµæ°´è™Ÿ] [æ—¥æœŸ...] [å¸‚å ´ä»£ç¢¼]
                # æˆ‘å€‘æŠŠ entry å…§çš„æ‰€æœ‰ç©ºç™½æš«æ™‚ç§»é™¤ä¾†æå–æ ¸å¿ƒè³‡è¨Š
                clean_entry = re.sub(r'\s+', ' ', entry.strip())
                parts = clean_entry.split(' ')
                
                # æµæ°´è™Ÿé€šå¸¸æ˜¯ç¬¬ä¸€æ®µ
                serial = parts[0]
                
                # å°‹æ‰¾æ—¥æœŸï¼šåœ¨ entry ä¸­å°‹æ‰¾ 7 ä½æ•¸å­—ä¸”å¾Œé¢ç·Šè·Ÿè‘— 1 æˆ– 8 (ä»£è¡¨æ—©æ™šå¸‚)
                # æ ¹æ“šæ‚¨çš„ç¯„ä¾‹ï¼š...114  11502101  11S00076...
                date_match = re.search(r'(\d{7})[18]\s', entry)
                if date_match:
                    date_str = date_match.group(1)
                    formatted_date = f"{date_str[:3]}/{date_str[3:5]}/{date_str[5:7]}"
                else:
                    continue

                # å¸‚å ´èˆ‡ç­‰ç´šæ¨™ç±¤
                s_pos = entry.find("S00076")
                level_code = entry[s_pos-2]
                level = grade_map.get(level_code, level_code)
                sub_id = entry[s_pos+6:s_pos+9]
                
                # æ•¸æ“šå€ï¼šä»¶æ•¸+é‡é‡+å–®åƒ¹
                nums = entry.split('+')
                if len(nums) >= 3:
                    pieces = int(re.sub(r'\D', '', nums[0][-3:]))
                    weight = int(re.sub(r'\D', '', nums[1]))
                    price_segment = nums[2].strip().split(' ')[0]
                    price = int(re.sub(r'\D', '', price_segment))
                    buyer = nums[-1].strip()[:4]

                    rows.append({
                        "æµæ°´è™Ÿ": serial,
                        "æ—¥æœŸ": formatted_date,
                        "ç­‰ç´š": level,
                        "å°ä»£": sub_id,
                        "ä»¶æ•¸": pieces,
                        "å…¬æ–¤": weight,
                        "å–®åƒ¹": price,
                        "è²·å®¶": buyer
                    })
            except:
                continue
    return rows

@st.cache_data(ttl=300)
def fetch_all_data():
    all_data = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        
        files = [f for f in r.json() if f['name'].upper().endswith('.SCP')]
        
        def process_file(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                text = res.content.decode("big5", errors="ignore")
                return parse_scp_content(text)
            return []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(process_file, files))
        
        for r_list in results:
            all_data.extend(r_list)
            
        df = pd.DataFrame(all_data)
        if not df.empty:
            # ã€é—œéµã€‘å‰”é™¤ç›¸åŒæµæ°´è™Ÿçš„è³‡æ–™ï¼Œåªé¡¯ç¤ºä¸€å€‹
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ", keep='first')
            df = df.sort_values(by=["æ—¥æœŸ", "å–®åƒ¹"], ascending=[False, False])
        return df
    except:
        return pd.DataFrame()

# --- ä¸»ä»‹é¢ ---
st.title("ğŸ“Š ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

df = fetch_all_data()

if not df.empty:
    st.sidebar.header("ğŸ› ï¸ æ•¸æ“šç¯©é¸")
    all_dates = sorted(df['æ—¥æœŸ'].unique(), reverse=True)
    sel_dates = st.sidebar.multiselect("ğŸ“… é¸æ“‡æ—¥æœŸ", all_dates)
    search_sub = st.sidebar.text_input("ğŸ” æœå°‹å°ä»£")
    show_serial = st.sidebar.checkbox("é¡¯ç¤ºåŸå§‹æµæ°´è™Ÿ", value=False)

    f_df = df.copy()
    if sel_dates: f_df = f_df[f_df['æ—¥æœŸ'].isin(sel_dates)]
    if search_sub: f_df = f_df[f_df['å°ä»£'].str.contains(search_sub)]

    c1, c2, c3 = st.columns(3)
    c1.metric("ä»¶æ•¸ç¸½è¨ˆ", f"{f_df['ä»¶æ•¸'].sum()} ä»¶")
    c2.metric("æœ€é«˜åƒ¹", f"{f_df['å–®åƒ¹'].max()} å…ƒ")
    c3.metric("è³‡æ–™ç­†æ•¸", f"{len(f_df)} ç­†")

    cols = ["æ—¥æœŸ", "ç­‰ç´š", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_serial: cols.insert(0, "æµæ°´è™Ÿ")
    
    st.dataframe(f_df[cols], use_container_width=True, height=600)
else:
    st.warning("âš ï¸ ç›®å‰é›²ç«¯å°šæœªæœ‰æ­£ç¢ºè§£æçš„è³‡æ–™ã€‚")