import streamlit as st
import pandas as pd
import re
import requests
from datetime import datetime
import concurrent.futures

st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…è³‡æ–™åº«", layout="wide")

REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

# --- è§£æé‚è¼¯ (ä¿æŒåŸæœ‰è§£æé‚è¼¯) ---
def parse_scp_content(content):
    raw_lines = content.split('    ')
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    for line in raw_lines:
        if "F22" in line and "S00076" in line:
            try:
                date_match = re.search(r"(\d{7,8}1)\s+\d{2}S00076", line)
                if date_match:
                    date_pos = date_match.start()
                    serial = line[:date_pos].strip().replace(" ", "")
                    # å¾æµæ°´è™Ÿæå–æ—¥æœŸ (å‡è¨­å‰ 7 ä½æ˜¯æ°‘åœ‹å¹´æœˆæ—¥)
                    record_date = f"{serial[:3]}/{serial[3:5]}/{serial[5:7]}"
                    
                    remaining = line[date_pos:]
                    s_pos = remaining.find("S00076")
                    level = grade_map.get(remaining[s_pos-2], remaining[s_pos-2])
                    sub_id = remaining[s_pos+6:s_pos+9]
                    
                    nums = line.split('+')
                    pieces = int(nums[0][-3:].replace(" ", "") or 0)
                    weight = int(nums[1].replace(" ", "") or 0)
                    price_raw = nums[2].strip().split(' ')[0]
                    price = int(price_raw[:-1] if price_raw else 0)
                    buyer = nums[5].strip()[:4]

                    rows.append({
                        "æµæ°´è™Ÿ": serial, "æ—¥æœŸ": record_date, "ç­‰ç´š": level, 
                        "å°ä»£": sub_id, "ä»¶æ•¸": pieces, "å…¬æ–¤": weight, 
                        "å–®åƒ¹": price, "è²·å®¶": buyer
                    })
            except: continue
    return rows

# --- æ‰¹æ¬¡æŠ“å– GitHub æ‰€æœ‰æª”æ¡ˆ ---
@st.cache_data(ttl=300)
def fetch_all_data():
    all_data = []
    try:
        # 1. å…ˆç²å–æª”æ¡ˆåˆ—è¡¨
        r = requests.get(API_URL)
        if r.status_code != 200: return []
        
        files = [f['download_url'] for f in r.json() if f['name'].endswith(('.SCP', '.txt'))]
        
        # 2. ä¸¦è¡Œä¸‹è¼‰æå‡é€Ÿåº¦
        def download_and_parse(url):
            res = requests.get(url)
            return parse_scp_content(res.content.decode("big5", errors="ignore")) if res.status_code == 200 else []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(download_and_parse, files))
        
        for r in results: all_data.extend(r)
        
        # 3. æ ¸å¿ƒåŠŸèƒ½ï¼šåˆ©ç”¨æµæ°´è™Ÿå»é‡
        df = pd.DataFrame(all_data)
        if not df.empty:
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ", keep="first")
            df = df.sort_values(by="æµæ°´è™Ÿ", ascending=False)
        return df
    except Exception as e:
        st.error(f"é€£ç·šå¤±æ•—: {e}")
        return pd.DataFrame()

# --- ä»‹é¢è¨­è¨ˆ ---
st.title("ğŸ“Š ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

df_all = fetch_all_data()

if not df_all.empty:
    # --- å´é‚Šæ¬„ç¯©é¸å€ ---
    st.sidebar.header("ğŸ› ï¸ ç¯©é¸æ¢ä»¶")
    
    # æ—¥æœŸå€é–“ç¯©é¸
    all_dates = sorted(df_all['æ—¥æœŸ'].unique(), reverse=True)
    date_range = st.sidebar.select_slider("é¸æ“‡æ—¥æœŸç¯„åœ", options=all_dates, value=(all_dates[-1], all_dates[0]))
    
    # å°ä»£æœå°‹
    search_sub = st.sidebar.text_input("ğŸ” æœå°‹å°ä»£ (å¦‚ 627)")
    
    # éæ¿¾è³‡æ–™
    mask = (df_all['æ—¥æœŸ'] >= date_range[0]) & (df_all['æ—¥æœŸ'] <= date_range[1])
    filtered_df = df_all[mask]
    
    if search_sub:
        filtered_df = filtered_df[filtered_df['å°ä»£'].str.contains(search_sub)]

    # --- æ•¸æ“šçµ±è¨ˆå¡ç‰‡ ---
    c1, c2, c3 = st.columns(3)
    c1.metric("ç•¶å‰ç¸½ä»¶æ•¸", f"{filtered_df['ä»¶æ•¸'].sum()} ä»¶")
    c2.metric("æœ€é«˜å–®åƒ¹", f"{filtered_df['å–®åƒ¹'].max()} å…ƒ")
    c3.metric("è³‡æ–™å¤©æ•¸", f"{len(filtered_df['æ—¥æœŸ'].unique())} å¤©")

    # --- è³‡æ–™è¡¨æ ¼ ---
    st.dataframe(filtered_df, use_container_width=True, height=500)
    
    # åŒ¯å‡ºåŠŸèƒ½
    st.download_button("ğŸ“¥ ä¸‹è¼‰ç›®å‰ç¯©é¸è³‡æ–™ (Excelæ ¼å¼)", 
                       filtered_df.to_csv(index=False).encode('utf-8-sig'),
                       "market_data.csv", "text/csv")
else:
    st.info("ç›®å‰é›²ç«¯å€‰åº«ä¸­æ²’æœ‰æœ‰æ•ˆçš„ SCP æª”æ¡ˆã€‚")