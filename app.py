import streamlit as st
import pandas as pd
import re
import requests
import concurrent.futures

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ç‡•å·¢å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«", layout="wide")

REPO_OWNER = "goodgorilla5"
REPO_NAME = "chaochao-catcher"
API_URL = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/"

try:
    GITHUB_TOKEN = st.secrets["github_token"]
except:
    st.error("âŒ è«‹è‡³ Streamlit Secrets è¨­å®š github_token")
    st.stop()

# --- æ ¸å¿ƒè§£æï¼šé‡å°é•·æµæ°´è™Ÿç²¾æº–åˆ‡ç‰‡ ---
def parse_scp_content(content):
    # ä½¿ç”¨ 4 å€‹ç©ºæ ¼åˆ†å‰²æ¯ä¸€ç­†ç´€éŒ„
    lines = content.split('    ')
    rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    
    for line in lines:
        # ç¢ºä¿æ˜¯ç‡•å·¢(S00076)ä¸”åŒ…å«èŠ­æ¨‚(F22)æˆ–ç›¸é—œä»£ç¢¼
        if "S00076" in line:
            try:
                # 1. å®šä½ S00076 æ¨™ç±¤
                s_pos = line.find("S00076")
                
                # 2. æŠ“å–æ—¥æœŸï¼šS00076 å‰æ–¹çš„ 7 ä½æ•¸å­— (ä¾‹å¦‚ 1150210)
                # æ ¹æ“š A11150210... æ ¼å¼ï¼Œæ—¥æœŸå‡ºç¾åœ¨ S00076 å‰ 9 åˆ°å‰ 2 ä½
                date_part = line[s_pos-9 : s_pos-2]
                
                if date_part.isdigit() and len(date_part) == 7:
                    formatted_date = f"{date_part[:3]}/{date_part[3:5]}/{date_part[5:7]}"
                    
                    # 3. æµæ°´è™Ÿï¼šå–æ•´è¡Œå‰ 30 ç¢¼ä½œç‚ºå”¯ä¸€ ID (ç”¨æ–¼å»é‡)
                    serial = line[:30].strip().replace(" ", "")

                    # 4. ç­‰ç´šèˆ‡å°ä»£ (ä½ç½®ç›¸å°æ–¼ S00076)
                    level_code = line[s_pos-2]
                    level = grade_map.get(level_code, level_code)
                    sub_id = line[s_pos+6:s_pos+9]
                    
                    # 5. æ•¸æ“šå€ (ä»¶æ•¸+é‡é‡+å–®åƒ¹)
                    # æ ¼å¼å¦‚: 003+00018+01400+000002520+6000+4218
                    nums = line.split('+')
                    # ä»¶æ•¸ï¼šåŠ è™Ÿå‰æœ€å¾Œä¸‰ç¢¼
                    pieces = int(re.sub(r'\D', '', nums[0][-3:]))
                    # é‡é‡ï¼šç¬¬ä¸€å€‹åŠ è™Ÿå¾Œ
                    weight = int(re.sub(r'\D', '', nums[1]))
                    # å–®åƒ¹ï¼šç¬¬äºŒå€‹åŠ è™Ÿå¾Œï¼Œæ‹¿æ‰éæ•¸å­—
                    price_raw = nums[2].split(' ')[0]
                    price = int(re.sub(r'\D', '', price_raw))
                    # è²·å®¶ï¼šæœ€å¾Œä¸€å€‹åŠ è™Ÿå¾Œ
                    buyer = nums[-1].strip()[:4]

                    rows.append({
                        "æµæ°´è™Ÿ": serial, "æ—¥æœŸ": formatted_date, "ç­‰ç´š": level, 
                        "å°ä»£": sub_id, "ä»¶æ•¸": pieces, "å…¬æ–¤": weight, 
                        "å–®åƒ¹": price, "è²·å®¶": buyer
                    })
            except: continue
    return rows

@st.cache_data(ttl=300)
def fetch_all_data():
    all_data = []
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    try:
        r = requests.get(API_URL, headers=headers)
        if r.status_code != 200: return pd.DataFrame()
        
        file_list = [f for f in r.json() if f['name'].upper().endswith(('.SCP', '.TXT'))]
        
        def download_and_parse(file_info):
            res = requests.get(file_info['download_url'], headers=headers)
            if res.status_code == 200:
                text = res.content.decode("big5", errors="ignore")
                # æ’é™¤ HTML å£æª”
                if "<html" in text.lower(): return []
                return parse_scp_content(text)
            return []

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(download_and_parse, file_list))
        
        for res in results: all_data.extend(res)
        df = pd.DataFrame(all_data)
        if not df.empty:
            df = df.drop_duplicates(subset="æµæ°´è™Ÿ")
            df = df.sort_values(by=["æ—¥æœŸ", "å–®åƒ¹"], ascending=[False, False])
        return df
    except: return pd.DataFrame()

# --- UI ä»‹é¢ ---
st.title("ğŸ“Š ç‡•å·¢-å°åŒ—è¡Œæƒ…å¤§æ•¸æ“šåº«")

df_all = fetch_all_data()

if not df_all.empty:
    st.sidebar.header("ğŸ› ï¸ æ•¸æ“šç¯©é¸")
    all_dates = sorted(df_all['æ—¥æœŸ'].unique(), reverse=True)
    selected_dates = st.sidebar.multiselect("ğŸ“… é¸æ“‡æ—¥æœŸ (é è¨­å…¨éƒ¨)", all_dates)
    search_sub = st.sidebar.text_input("ğŸ” æœå°‹å°ä»£")
    show_serial = st.sidebar.checkbox("é¡¯ç¤ºåŸå§‹æµæ°´è™Ÿ", value=False)

    filtered_df = df_all.copy()
    if selected_dates: filtered_df = filtered_df[filtered_df['æ—¥æœŸ'].isin(selected_dates)]
    if search_sub: filtered_df = filtered_df[filtered_df['å°ä»£'].str.contains(search_sub)]

    c1, c2, c3 = st.columns(3)
    c1.metric("ç¸½ä»¶æ•¸", f"{filtered_df['ä»¶æ•¸'].sum()} ä»¶")
    c2.metric("æœ€é«˜å–®åƒ¹", f"{filtered_df['å–®åƒ¹'].max()} å…ƒ")
    c3.metric("è³‡æ–™ç­†æ•¸", f"{len(filtered_df)} ç­†")

    # æ§åˆ¶éš±è—æµæ°´è™Ÿ
    display_cols = ["æ—¥æœŸ", "ç­‰ç´š", "å°ä»£", "ä»¶æ•¸", "å…¬æ–¤", "å–®åƒ¹", "è²·å®¶"]
    if show_serial: display_cols.insert(0, "æµæ°´è™Ÿ")
    
    st.dataframe(filtered_df[display_cols], use_container_width=True, height=600)
else:
    st.warning("âš ï¸ é›²ç«¯ç›®å‰å°šæœªæœ‰æ­£ç¢ºè§£æçš„è³‡æ–™ã€‚è«‹ç¢ºèª GitHub ä¸­çš„ SCP æª”æ¡ˆå·²æ›´æ–°ç‚ºæ­£ç¢ºç‰ˆæœ¬ã€‚")